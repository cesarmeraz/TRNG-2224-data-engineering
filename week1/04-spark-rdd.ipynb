{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc5d93d",
   "metadata": {},
   "source": [
    "# RDD\n",
    "\n",
    "an RDD is the fundamental data structure of Apache Spark. It's a fault-tolerant, distributed collection of elements that can be operated on in parallel.\n",
    "\n",
    "**Key Characteristics:**\n",
    "\n",
    "- Immutable\n",
    "- Lazy evaluation\n",
    "- Fault tolerant (via lineage info)\n",
    "- Partitioned across cluster nodes\n",
    "- Can be cached in memory\n",
    "\n",
    "### SparkContext and SparkConf\n",
    "\n",
    "\n",
    "SparkContext is the entry point for Spark functionality.\n",
    "\n",
    "#### `SparkConf`\n",
    "\n",
    "- Configuration for Spark application\n",
    "\n",
    "**Common settings:**\n",
    "\n",
    "- setMaster(\"local[*]\") – Use local mode with all cores\n",
    "- setAppName(\"RDDExample\") – Application name\n",
    "\n",
    "### transformations\n",
    "\n",
    "Transformations create a new RDD from an existing one. They are lazy – not executed until an action is triggered.\n",
    "\n",
    "| Transformation  | Description                                          |\n",
    "| --------------- | ---------------------------------------------------- |\n",
    "| `map(func)`     | Returns a new RDD by applying `func` to each element |\n",
    "| `filter(func)`  | Filters elements for which `func` returns true       |\n",
    "| `flatMap(func)` | Like map but flattens the result                     |\n",
    "| `distinct()`    | Removes duplicates                                   |\n",
    "| `union(rdd)`    | Combines two RDDs                                    |\n",
    "| `groupByKey()`  | Groups values with same key                          |\n",
    "| `reduceByKey()` | Aggregates values with same key using a function     |\n",
    "| `sortBy(func)`  | Sorts RDD by computed key                            |\n",
    "\n",
    "\n",
    "### actions\n",
    "\n",
    "Actions trigger computation and return results or write data.\n",
    "\n",
    "| Action             | Description                            |\n",
    "| ------------------ | -------------------------------------- |\n",
    "| `collect()`        | Returns all elements to driver         |\n",
    "| `count()`          | Returns number of elements             |\n",
    "| `first()`          | Returns first element                  |\n",
    "| `take(n)`          | Returns first `n` elements             |\n",
    "| `reduce(func)`     | Reduces elements using binary operator |\n",
    "| `saveAsTextFile()` | Writes RDD to text files               |\n",
    "\n",
    "\n",
    "\n",
    "reference - [spark rdd docs](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b80fcd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in /workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages (from pyspark) (0.10.9.9)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark\n",
    "\n",
    "# pyspark 4.0.0 - jdk 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f771c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Invalid requirement: 'pyspark=3.3.0'\n",
      "Hint: = is not a valid operator. Did you mean == ?\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install pyspark=3.3.0 \n",
    "\n",
    "# pyspark 3.3.* - jdk 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3c1b247f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=salesDemo, master=local[*]) created by __init__ at /tmp/ipykernel_6478/4167189320.py:7 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkContext, SparkConf\n\u001b[1;32m      5\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msalesDemo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/pyspark/core/context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n\u001b[0;32m--> 205\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    208\u001b[0m         master,\n\u001b[1;32m    209\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    220\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/pyspark/core/context.py:457\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    454\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    462\u001b[0m             currentAppName,\n\u001b[1;32m    463\u001b[0m             currentMaster,\n\u001b[1;32m    464\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    465\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    466\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    467\u001b[0m         )\n\u001b[1;32m    468\u001b[0m     )\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=salesDemo, master=local[*]) created by __init__ at /tmp/ipykernel_6478/4167189320.py:7 "
     ]
    }
   ],
   "source": [
    "# SparkContext and SparkConf\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"salesDemo\").setMaster(\"local[*]\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1caba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e73ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1001,Furniture,69',\n",
       " '1002,Clothing,78',\n",
       " '1003,Electronics,821',\n",
       " '1004,Furniture,357',\n",
       " '1005,Furniture,429',\n",
       " '1006,Furniture,540',\n",
       " '1007,Books,724',\n",
       " '1008,Electronics,770',\n",
       " '1009,Furniture,503',\n",
       " '1010,Clothing,331',\n",
       " '1011,Clothing,227',\n",
       " '1012,Toys,400',\n",
       " '1013,Clothing,459',\n",
       " '1014,Electronics,987',\n",
       " '1015,Books,627',\n",
       " '1016,Electronics,481',\n",
       " '1017,Books,314',\n",
       " '1018,Furniture,803',\n",
       " '1019,Electronics,453',\n",
       " '1020,Toys,800',\n",
       " '1021,Clothing,894',\n",
       " '1022,Furniture,176',\n",
       " '1023,Electronics,352',\n",
       " '1024,Electronics,930',\n",
       " '1025,Toys,584',\n",
       " '1026,Electronics,248',\n",
       " '1027,Toys,243',\n",
       " '1028,Toys,861',\n",
       " '1029,Books,229',\n",
       " '1030,Toys,618',\n",
       " '1031,Electronics,246',\n",
       " '1032,Toys,829',\n",
       " '1033,Clothing,329',\n",
       " '1034,Clothing,427',\n",
       " '1035,Toys,101',\n",
       " '1036,Clothing,330',\n",
       " '1037,Furniture,567',\n",
       " '1038,Toys,542',\n",
       " '1039,Electronics,941',\n",
       " '1040,Clothing,87',\n",
       " '1041,Furniture,514',\n",
       " '1042,Books,668',\n",
       " '1043,Electronics,545',\n",
       " '1044,Toys,558',\n",
       " '1045,Electronics,616',\n",
       " '1046,Books,412',\n",
       " '1047,Furniture,527',\n",
       " '1048,Furniture,364',\n",
       " '1049,Clothing,545',\n",
       " '1050,Books,500',\n",
       " '1051,Furniture,168',\n",
       " '1052,Clothing,57',\n",
       " '1053,Electronics,483',\n",
       " '1054,Toys,316',\n",
       " '1055,Toys,693',\n",
       " '1056,Furniture,860',\n",
       " '1057,Furniture,727',\n",
       " '1058,Toys,836',\n",
       " '1059,Electronics,59',\n",
       " '1060,Furniture,734',\n",
       " '1061,Furniture,586',\n",
       " '1062,Electronics,149',\n",
       " '1063,Books,85',\n",
       " '1064,Furniture,653',\n",
       " '1065,Electronics,651',\n",
       " '1066,Furniture,425',\n",
       " '1067,Electronics,366',\n",
       " '1068,Furniture,633',\n",
       " '1069,Clothing,883',\n",
       " '1070,Furniture,879',\n",
       " '1071,Toys,419',\n",
       " '1072,Toys,678',\n",
       " '1073,Toys,800',\n",
       " '1074,Clothing,440',\n",
       " '1075,Electronics,356',\n",
       " '1076,Clothing,797',\n",
       " '1077,Clothing,353',\n",
       " '1078,Electronics,845',\n",
       " '1079,Toys,294',\n",
       " '1080,Clothing,568',\n",
       " '1081,Electronics,881',\n",
       " '1082,Furniture,645',\n",
       " '1083,Clothing,498',\n",
       " '1084,Furniture,686',\n",
       " '1085,Toys,468',\n",
       " '1086,Clothing,142',\n",
       " '1087,Books,409',\n",
       " '1088,Toys,683',\n",
       " '1089,Electronics,218',\n",
       " '1090,Toys,183',\n",
       " '1091,Clothing,647',\n",
       " '1092,Electronics,778',\n",
       " '1093,Electronics,186',\n",
       " '1094,Toys,587',\n",
       " '1095,Books,262',\n",
       " '1096,Books,251',\n",
       " '1097,Clothing,716',\n",
       " '1098,Electronics,776',\n",
       " '1099,Clothing,495',\n",
       " '1100,Toys,301']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the file into RDD\n",
    "\n",
    "sales_raw = sc.textFile(\"file:////workspace/TRNG-2224-data-engineering/week1/datasets/sales.txt\")\n",
    "\n",
    "sales_raw.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert each line into a tuple (ProductID, Category, Amount)\n",
    "\n",
    "records = sales_raw.map(lambda x: x.split(\",\")).map(lambda x: (int(x[0]), x[1], int(x[2])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a (Category, Amount) RDD\n",
    "\n",
    "category_sales = records.map(lambda x: (x[1], x[2]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22401f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Total sales by category\n",
    "\n",
    "total_sales_by_category = category_sales.reduceByKey(lambda x,y: x+y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63568bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Furniture', 538.4090909090909),\n",
       " ('Clothing', 443.0),\n",
       " ('Electronics', 547.4166666666666),\n",
       " ('Books', 407.3636363636364)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Average sale per category\n",
    "\n",
    "average_sales_by_category = category_sales.mapValues(lambda x: (x, 1)).reduceByKey(lambda x,y: (x[0] + y[0], x[1]+ y[1])).mapValues(lambda x: x[0]/x[1])\n",
    "\n",
    "average_sales_by_category.take(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96717e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1014, 'Electronics', 987)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Highest transaction\n",
    "\n",
    "max_tran = records.max(key=lambda x : x[2])\n",
    "\n",
    "max_tran\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e032282f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Electronics', 13138)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# highest selling category\n",
    "\n",
    "highest_selling_category = total_sales_by_category.max(key=lambda x: x[1] )\n",
    "\n",
    "highest_selling_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0986f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Furniture', 11845),\n",
       " ('Clothing', 9303),\n",
       " ('Electronics', 13138),\n",
       " ('Toys', 11794)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7: Categories with sales above 5000\n",
    "\n",
    "high_selling_cat_5k = total_sales_by_category.filter(lambda x: x[1] > 5000)\n",
    "\n",
    "high_selling_cat_5k.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d2cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sales by cat\n",
      "[('Furniture', 11845), ('Clothing', 9303), ('Electronics', 13138), ('Books', 4481), ('Toys', 11794)]\n",
      "Avg sales per cat\n",
      "[('Furniture', 538.4090909090909), ('Clothing', 443.0), ('Electronics', 547.4166666666666), ('Books', 407.3636363636364), ('Toys', 536.0909090909091)]\n"
     ]
    }
   ],
   "source": [
    "# print final results\n",
    "\n",
    "print(\"Total sales by cat\")\n",
    "print(total_sales_by_category.collect())\n",
    "\n",
    "print(\"Avg sales per cat\")\n",
    "\n",
    "print(average_sales_by_category.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65a088",
   "metadata": {},
   "source": [
    "**Assignment:**\n",
    "\n",
    "#1. find all product IDs where the amount is greater than 900."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819bd911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1014, 1024, 1039]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "high_end_products = records.filter(lambda x: x[2] > 900)\n",
    "high_end_products_ids = high_end_products.map(lambda x: x[0])\n",
    "print(high_end_products_ids.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f36081",
   "metadata": {},
   "source": [
    "2. Find all transactions that belong to the “Furniture” category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a37307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1001, 'Furniture', 69), (1004, 'Furniture', 357), (1005, 'Furniture', 429), (1006, 'Furniture', 540), (1009, 'Furniture', 503), (1018, 'Furniture', 803), (1022, 'Furniture', 176), (1037, 'Furniture', 567), (1041, 'Furniture', 514), (1047, 'Furniture', 527), (1048, 'Furniture', 364), (1051, 'Furniture', 168), (1056, 'Furniture', 860), (1057, 'Furniture', 727), (1060, 'Furniture', 734), (1061, 'Furniture', 586), (1064, 'Furniture', 653), (1066, 'Furniture', 425), (1068, 'Furniture', 633), (1070, 'Furniture', 879), (1082, 'Furniture', 645), (1084, 'Furniture', 686)]\n"
     ]
    }
   ],
   "source": [
    "furniture_transactions = records.filter(lambda x: x[1] == 'Furniture' )\n",
    "print(furniture_transactions.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c920434",
   "metadata": {},
   "source": [
    "3. Count how many transactions belong to the “Electronics” category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb9d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "electronics_transactions = records.filter(lambda x: x[1] == 'Electronics' )\n",
    "electronics_transactions.collect()\n",
    "count_electronics_transactions = electronics_transactions.count()\n",
    "print(count_electronics_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ff367",
   "metadata": {},
   "source": [
    "4. Find average amount for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40dfe55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Furniture', 538.4090909090909),\n",
       " ('Clothing', 443.0),\n",
       " ('Electronics', 547.4166666666666),\n",
       " ('Books', 407.3636363636364),\n",
       " ('Toys', 536.0909090909091)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_sales_by_category.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a3b6a9",
   "metadata": {},
   "source": [
    "\n",
    "5. Find the highest amount and the corresponding product ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc74437e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest amount 987 for product id 1014\n"
     ]
    }
   ],
   "source": [
    "max_tran = records.max(key=lambda x : x[2])\n",
    "print(f\"Highest amount {max_tran[2]} for product id {max_tran[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea368ac8",
   "metadata": {},
   "source": [
    "6. Find the total number of unique categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c4ed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique category count: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique category count: {average_sales_by_category.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a99406",
   "metadata": {},
   "source": [
    "7. For each category, find the product ID with the highest sale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e102d",
   "metadata": {},
   "source": [
    "8. Count how many products were sold for less than 300.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6d2a0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "cheap_products = records.filter(lambda x: x[2] < 300)\n",
    "print(cheap_products.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc1f1dd",
   "metadata": {},
   "source": [
    "9. Sort the transactions by amount in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "acfd063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1014, 'Electronics', 987), (1039, 'Electronics', 941), (1024, 'Electronics', 930), (1021, 'Clothing', 894), (1069, 'Clothing', 883), (1081, 'Electronics', 881), (1070, 'Furniture', 879), (1028, 'Toys', 861), (1056, 'Furniture', 860), (1078, 'Electronics', 845), (1058, 'Toys', 836), (1032, 'Toys', 829), (1003, 'Electronics', 821), (1018, 'Furniture', 803), (1020, 'Toys', 800), (1073, 'Toys', 800), (1076, 'Clothing', 797), (1092, 'Electronics', 778), (1098, 'Electronics', 776), (1008, 'Electronics', 770), (1060, 'Furniture', 734), (1057, 'Furniture', 727), (1007, 'Books', 724), (1097, 'Clothing', 716), (1055, 'Toys', 693), (1084, 'Furniture', 686), (1088, 'Toys', 683), (1072, 'Toys', 678), (1042, 'Books', 668), (1064, 'Furniture', 653), (1065, 'Electronics', 651), (1091, 'Clothing', 647), (1082, 'Furniture', 645), (1068, 'Furniture', 633), (1015, 'Books', 627), (1030, 'Toys', 618), (1045, 'Electronics', 616), (1094, 'Toys', 587), (1061, 'Furniture', 586), (1025, 'Toys', 584), (1080, 'Clothing', 568), (1037, 'Furniture', 567), (1044, 'Toys', 558), (1043, 'Electronics', 545), (1049, 'Clothing', 545), (1038, 'Toys', 542), (1006, 'Furniture', 540), (1047, 'Furniture', 527), (1041, 'Furniture', 514), (1009, 'Furniture', 503), (1050, 'Books', 500), (1083, 'Clothing', 498), (1099, 'Clothing', 495), (1053, 'Electronics', 483), (1016, 'Electronics', 481), (1085, 'Toys', 468), (1013, 'Clothing', 459), (1019, 'Electronics', 453), (1074, 'Clothing', 440), (1005, 'Furniture', 429), (1034, 'Clothing', 427), (1066, 'Furniture', 425), (1071, 'Toys', 419), (1046, 'Books', 412), (1087, 'Books', 409), (1012, 'Toys', 400), (1067, 'Electronics', 366), (1048, 'Furniture', 364), (1004, 'Furniture', 357), (1075, 'Electronics', 356), (1077, 'Clothing', 353), (1023, 'Electronics', 352), (1010, 'Clothing', 331), (1036, 'Clothing', 330), (1033, 'Clothing', 329), (1054, 'Toys', 316), (1017, 'Books', 314), (1100, 'Toys', 301), (1079, 'Toys', 294), (1095, 'Books', 262), (1096, 'Books', 251), (1026, 'Electronics', 248), (1031, 'Electronics', 246), (1027, 'Toys', 243), (1029, 'Books', 229), (1011, 'Clothing', 227), (1089, 'Electronics', 218), (1093, 'Electronics', 186), (1090, 'Toys', 183), (1022, 'Furniture', 176), (1051, 'Furniture', 168), (1062, 'Electronics', 149), (1086, 'Clothing', 142), (1035, 'Toys', 101), (1040, 'Clothing', 87), (1063, 'Books', 85), (1002, 'Clothing', 78), (1001, 'Furniture', 69), (1059, 'Electronics', 59), (1052, 'Clothing', 57)]\n"
     ]
    }
   ],
   "source": [
    "sorted = records.sortBy(lambda x: x[2], False)\n",
    "print(sorted.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3292be",
   "metadata": {},
   "source": [
    "### Shared Variables\n",
    "\n",
    "When you pass a function (like in map or reduce) to Spark, that function runs on different machines in the cluster — not on your local driver.\n",
    "\n",
    "**By default:**\n",
    "\n",
    "- Spark makes separate copies of any variable you use inside those functions.\n",
    "- So, if a task changes a variable on the executor, that change will not reflect back in your driver program.\n",
    "\n",
    "This is done to keep things fast and distributed — but it also means you can not just update normal variables across tasks.\n",
    "\n",
    "**challenges:**\n",
    "\n",
    "You want to count how many rows have Amount > 500 using this code:\n",
    "\n",
    "```py\n",
    "count = 0\n",
    "\n",
    "def increment_count(x):\n",
    "    count += 1 \n",
    "\n",
    "records.filter(lambda x: int(x[2]) > 500).foreach(increment_count)\n",
    "print(count)\n",
    "\n",
    "```\n",
    "\n",
    "This will not work because each machine updates its own copy of count not the original one in the driver.\n",
    "\n",
    "To solve such issues, spark provides two types of shared variables:\n",
    "\n",
    "1. **Broadcast Variables:** a read-only variable that can be cached on each machine (executor). Used to efficiently share large data (like lookup tables) with all tasks without copying it multiple times.\n",
    "2. **Accumulators:** variables used to safely implement counters or sums across mulitple worker nodes. You can only add to them (not read or subtract inside tasks). The final value is only accessible on the driver after an action is executed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f9bdaca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "\n",
    "region_sales = sc.textFile(\"file:///workspace/TRNG-2224-data-engineering/week1/datasets/region_sales.txt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e9b697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a broadcast variable for region-level config\n",
    "\n",
    "records_rs = region_sales.map(lambda x: x.split(\",\")).map(lambda x: (int(x[0]), x[1], x[2], int(x[3])))\n",
    "\n",
    "regions = {\"North\": \"N\", \"South\": \"S\", \"East\": \"E\", \"West\": \"W\"}\n",
    "\n",
    "region_broadcast = sc.broadcast(regions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "009c5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an accumulator to count high-value transactions\n",
    "\n",
    "high_value_count = sc.accumulator(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "49b4ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_region_and_count(record):\n",
    "    pid, region, category, amount = record\n",
    "    if amount > 800:\n",
    "        high_value_count.add(1)\n",
    "    return (pid, region_broadcast.value[region], category, amount)\n",
    "\n",
    "enriched_records = records_rs.map(enrich_region_and_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e79a5f06",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o675.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/workspace/TRNG-2224-data-engineering/week1/filtered_high_value_sales already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:303)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:73)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1094)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1092)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1065)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1029)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1011)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1010)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:967)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1631)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1631)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1617)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1617)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:565)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save filtered RDD to a new file\u001b[39;00m\n\u001b[1;32m      3\u001b[0m filtered \u001b[38;5;241m=\u001b[39m enriched_records\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m800\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mfiltered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfiltered_high_value_sales\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/pyspark/core/rdd.py:3294\u001b[0m, in \u001b[0;36mRDD.saveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   3292\u001b[0m     keyed\u001b[38;5;241m.\u001b[39m_jrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mBytesToString())\u001b[38;5;241m.\u001b[39msaveAsTextFile(path, compressionCodec)\n\u001b[1;32m   3293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3294\u001b[0m     \u001b[43mkeyed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o675.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/workspace/TRNG-2224-data-engineering/week1/filtered_high_value_sales already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:303)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:73)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1094)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1092)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1065)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1029)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1011)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1010)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:967)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1631)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1631)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1617)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1617)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:565)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "# Save filtered RDD to a new file\n",
    "\n",
    "filtered = enriched_records.filter(lambda x: x[3] >800)\n",
    "\n",
    "filtered.map(lambda x: \",\".join(map(str,x))).coalesce(1).saveAsTextFile(\"filtered_high_value_sales\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1fd45b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_value_count.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33195e4f",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "**tasks:**\n",
    "\n",
    "1.  Load the Dataset\n",
    "2. Broadcast Variable: Category Discounts\n",
    "\n",
    "```py\n",
    "{\"Electronics\": 0.10, \"Furniture\": 0.15, \"Clothing\": 0.05, \"Books\": 0.20}\n",
    "```\n",
    "\n",
    "3. calculate and return the rdd:\n",
    "\n",
    "```py\n",
    "(ProductID, Region, Category, OriginalAmount, DiscountApplied)\n",
    "```\n",
    "4. Accumulator: Count Transactions Below ₹300\n",
    "5. Filter and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a832dbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "ds_raw = sc.textFile(\"file:////workspace/TRNG-2224-data-engineering/week1/datasets/region_sales.txt\")\n",
    "ds_raw.collect()\n",
    "region_sales_records = ds_raw.map(lambda x: x.split(\",\")).map(lambda x: (int(x[0]), x[1], x[2], int(x[3])))\n",
    "\n",
    "cat_discounts = {\"Electronics\": 0.10, \"Furniture\": 0.15, \"Clothing\": 0.05, \"Books\": 0.20}\n",
    "cat_discounts_broadcast = sc.broadcast(cat_discounts)\n",
    "low_trans_count = sc.accumulator(0)\n",
    "\n",
    "def apply_discount(record):\n",
    "    pid, region, category, amount = record\n",
    "    discount = amount * cat_discounts_broadcast.value[category]\n",
    "    new_record = (pid, region, category, amount, amount - discount)\n",
    "    if (amount - discount) < 300:\n",
    "        low_trans_count.add(1)\n",
    "    return new_record\n",
    "\n",
    "discounted_records = region_sales_records.map(apply_discount)\n",
    "filtered = discounted_records.filter(lambda x: x[4] < 300)\n",
    "print(low_trans_count.value)\n",
    "filtered.map(lambda x: \",\".join(map(str,x))).coalesce(1).saveAsTextFile(\"filtered_low_sale_discounts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
